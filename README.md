README — Weather Forecasting With Deep Learning

This project focuses on building a 7-day (168-hour) weather forecasting system using several deep learning architectures. All experiments were run on Modal, which provided GPU acceleration and persistent storage through Modal volumes. The overall workflow included data preprocessing, model training, model evaluation, hyperparameter tuning, and final result visualization.

The process began with a dedicated preprocessing script that transformed the raw weather dataset into a machine-learning-ready format. Each sample was constructed using a 10-day input window with hourly readings, giving models 240 timesteps of historical information. The goal was to predict the following 168 hours (seven days). The dataset contained a wide range of meteorological and temporal features—temperature, relative humidity, wind speed, wind direction, soil measurements, and cyclical timestamp encodings—while the prediction targets consisted of the six primary weather variables. All processed data (X.npy and Y.npy) was stored inside a Modal volume so that every model could access the same standardized dataset.

With the data prepared, three different neural network architectures were implemented and trained independently: an LSTM Seq2Seq model with attention, a Temporal Convolutional Neural Network (TCN), and a Transformer-based model. Each architecture had its own training script designed to load data directly from the shared Modal volume, train the model on an A100 GPU, and store all checkpoints and final weights back into the volume. This setup allowed the team to experiment with model-specific improvements while maintaining a consistent data pipeline.

For evaluation, each team member developed their own testing script tailored to their model. This allowed independent experimentation, custom visualization, and deeper analysis of prediction behavior. Every script saved its resulting performance metrics and plots back into the shared Modal volume, helping compare approaches in a consistent way. After all individual evaluations were completed, a unified benchmarking script was created to test all models side-by-side. It compared LSTM, TCN, and Transformer architectures across several metrics—including MAE, MSE, RMSE, R², and full forecast horizon error analysis—to determine which model performed best overall.

Once the strongest architecture was identified, the team used a custom hyperparameter optimization (HPO) script to fine-tune the model further. This process explored variations in learning rate, hidden sizes, dropout, sequence modeling parameters, teacher forcing, and other training hyperparameters. The best-performing tuned model was saved back into the volume for final use.

Finally, a dedicated plotting script was used to generate clean and comprehensive visualizations, including forecast versus actual curves, horizon MAE plots, error distributions, per-feature R² scores, loss curves, and sample prediction timelines. The script was designed to delete all previous plots before producing new ones, ensuring that every evaluation run generated a fresh, reproducible set of visual outputs.

Overall, this project demonstrates a complete deep learning pipeline for multi-step weather forecasting—from preprocessing raw data all the way to model comparison, fine-tuning, and visualization—using Modal as a scalable and collaborative compute platform.
